{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNGzdZrKbrA7qdLGNPtUIHn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**1.1**"],"metadata":{"id":"EAcIG_Ms3aBV"}},{"cell_type":"markdown","source":["Logistic regression models the probability of a binary outcome using the sigmoid function applied to a linear model:\n","\n","$\\hat{y}_i = \\sigma(w^\\top x_i + b)$\n","\n","$\\sigma(z) = \\frac{1}{1 + e^{-z}}$"],"metadata":{"id":"IEDp2c9D3aEF"}},{"cell_type":"markdown","source":["Because the target variable satisfies\n","$y_i ∈ {0,1}$, each label is modeled as a Bernoulli random variable with parameter\n","$y^i$."],"metadata":{"id":"leCmxJ-83aGn"}},{"cell_type":"markdown","source":["Likelihood of a single obersvation:\n","\n","$p(y_i \\mid x_i; w, b) = \\hat{y}_i^{y_i}(1 - \\hat{y}_i)^{1 - y_i}$\n","\n","Assuming the data points are conditionally independent, the likelihood of the full dataset\n","$D={(xi,yi)}$ is:\n","\n","$p(D \\mid \\theta) = \\prod_{i=1}^{n} \\hat{y}_i^{y_i}(1 - \\hat{y}_i)^{1 - y_i}$\n","\n","$\\theta = (w, b)$"],"metadata":{"id":"v7v37FNE_NEO"}},{"cell_type":"markdown","source":["**MLE**\n","\n","Maximum Likelihood Estimation chooses parameters that maximize the probability of observing the data:\n","\n","$\\hat{\\theta}_{\\text{MLE}} = \\arg\\max_{\\theta} \\; p(D \\mid \\theta)$\n","\n","Taking the logarithm of the likelihood gives the log-likelihood:\n","\n","$\\ell(\\theta) = \\sum_{i=1}^{n} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i)\\log(1 - \\hat{y}_i)\n","\\right]$\n","\n","Minimizing the negative log-likelihood yields the binary cross-entropy loss:\n","\n","$J_{\\text{MLE}}(\\theta) = -\\ell(\\theta)$"],"metadata":{"id":"7KNiIucJ_NHE"}},{"cell_type":"markdown","source":["**MAP**\n","\n","Maximum A Posteriori estimation includes a prior distribution over the parameters:\n","\n","$\\hat{\\theta}_{\\text{MAP}} = \\arg\\max_{\\theta} \\; p(\\theta \\mid D)$\n","\n","Using Bayes’ rule, this can be written as:\n","\n","$\\hat{\\theta}_{\\text{MAP}}\n","= \\arg\\max_{\\theta}\n","\\left[\n","\\log p(D \\mid \\theta) + \\log p(\\theta)\n","\\right]$\n","\n","MAP minimizes this objective:\n","\n","$J_{\\text{MAP}}(\\theta)\n","=\n","-\\ell(\\theta) - \\log p(\\theta)$\n","\n","\n","**The prior term log⁡p(θ) acts as a regularization term. Therefore, MAP estimation corresponds to regularized logistic regression, while MLE corresponds to the unregularized case.**\n","\n","https://www.geeksforgeeks.org/data-science/mle-vs-map/"],"metadata":{"id":"tuYQTziuAPWf"}},{"cell_type":"markdown","source":["**1.2**"],"metadata":{"id":"iXtfBayDBk01"}},{"cell_type":"markdown","source":["The machine learning problem considered for this assignment binary loan default prediction. Given borrower and loan features\n","$x∈R^d$, such as income, debt-to-income ratio, credit utilization, and prior delinquencies, the target variable is defined as:"],"metadata":{"id":"XEl8QLPdBk5R"}},{"cell_type":"markdown","source":["y = 1, if the borrower defaults\n","y = 0, otherwise.\n","\n","Logistic regression is the appropriate model for this task because it directly estimates the probability $p(y=1∣x)$, which is needed for risk-based decision making. In addition, its coefficients are easily interpretable, making it suitable for financial applications where model interpretability is important.\n","\n","An alternative linear classifier is the linear Support Vector Machine (SVM). While both models learn linear decision boundaries, logistic regression optimizes a likelihood-based loss, whereas SVM focuses on maximizing the margin.\n","\n","https://www.geeksforgeeks.org/machine-learning/support-vector-machine-algorithm/"],"metadata":{"id":"SIyo6MrmBk7S"}},{"cell_type":"markdown","source":["**1.3**\n","\n"],"metadata":{"id":"SnMSES3CCcqX"}},{"cell_type":"markdown","source":["For this task, each data point consists of a feature vector  $x_i∈R^d$ representing borrower and loan characteristics, such as income, debt-to-income ratio, credit utilization, and prior delinquencies. The corresponding target variable  $y_i∈{0,1}$ indicates whether the borrower defaults on the loan.\n","\n","The logistic regression model estimates the probability of default as:\n","\n","$p(y_i​ = 1∣ x_i​)=σ(w^\\top x_i​ + b)$.\n","\n","Here, the parameter vector wcaptures the influence of each feature on the log-odds of default, while the bias term b represents the baseline default risk.\n","\n","The derivation in Task 1.1 assumes that observations are independent and that the binary outcomes follow a Bernoulli distribution. Additionally, logistic regression assumes a linear relationship between the features and the log-odds of the outcome. In practice, this implies that features are appropriately scaled and that strong multicollinearity is addressed."],"metadata":{"id":"SqZgV2xdCcsa"}},{"cell_type":"markdown","source":[],"metadata":{"id":"B-LxNnK3CcuZ"}},{"cell_type":"markdown","source":[],"metadata":{"id":"PFjlIE9WCcy3"}}]}